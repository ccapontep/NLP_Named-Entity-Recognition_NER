{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "trainer_SenseTag_embed.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1yMOReyGAiWg",
        "colab_type": "text"
      },
      "source": [
        "# Setup of File"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fdKImX5FBs7j",
        "colab_type": "text"
      },
      "source": [
        "The following are done in this section:\n",
        "\n",
        "*   Mount Drive\n",
        "*   Download/Import Libraries\n",
        "*   Activate Environment\n",
        "*   Locate Working Dir and Dataset Files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3cCVNyvF-jKv",
        "colab_type": "code",
        "outputId": "dc43229f-273e-430d-d015-76291d89f865",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rGT7I5ei-okA",
        "colab_type": "code",
        "outputId": "4f02972d-9bac-4ba9-b34a-fa3ebe172961",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "# %cd gdrive/My\\ Drive/NLP_HW1/nlp2020-hw1/hw1/stud # personal\n",
        "%cd gdrive/My\\ Drive/nlp2020-hw1/hw1/stud\n",
        "!ls\n",
        "  "
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/.shortcut-targets-by-id/1QcOyZ4M-V21hM1AQD_BOlsVRd3FVVVUA/NLP_HW1/nlp2020-hw1/hw1/stud\n",
            "babelnet2wordnet.tsv  LocOrg_samples.txt\n",
            "images\t\t      Loc_samples.txt\n",
            "implementation.py     Org_samples.txt\n",
            "__init__.py\t      trainer_3_withoutPreTrainedEmbed.ipynb\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m6BBDeFcAxJW",
        "colab_type": "code",
        "outputId": "bc5d8c59-de38-46fd-9a94-c1d3c6e61d78",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "# Setup Client and activate new environment \n",
        "\n",
        "# !source activate nlp2020-hw1\n",
        "import sys\n",
        "!{sys.executable} -m pip install conllu"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting conllu\n",
            "  Downloading https://files.pythonhosted.org/packages/a8/03/4a952eb39cdc8da80a6a2416252e71784dda6bf9d726ab98065fff2aeb73/conllu-2.3.2-py2.py3-none-any.whl\n",
            "Installing collected packages: conllu\n",
            "Successfully installed conllu-2.3.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5R_w7ApW7829",
        "colab_type": "code",
        "outputId": "75d624b0-9e51-45c3-da2f-7800866f2125",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "source": [
        "# !source activate nlp2020-hw1\n",
        "import os, pickle\n",
        "import numpy as np\n",
        "import itertools\n",
        "from conllu import parse \n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "from collections import Counter\n",
        "from collections import defaultdict \n",
        "from torch.utils.data import DataLoader\n",
        "from gensim.models import KeyedVectors\n",
        "\n",
        "# For POS tag pre-processing\n",
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet as wn\n",
        "import csv\n",
        "from nltk.wsd import lesk\n",
        "\n",
        "\n",
        "# For plotting results:\n",
        "import seaborn as sn\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
        "\n",
        "\n",
        "# Get path for each file type\n",
        "directory = os.getcwd()\n",
        "dataset_dir = os.path.join(directory, \"..\", \"..\", \"data\")\n",
        "model_dir = os.path.join(directory, \"..\", \"..\", \"model\")\n",
        "\n",
        "file_train = os.path.join(dataset_dir, \"train.tsv\")\n",
        "file_dev = os.path.join(dataset_dir, \"dev.tsv\")\n",
        "file_test = os.path.join(dataset_dir, \"test.tsv\")\n",
        "embed_file = os.path.join(model_dir, \"embeddings.vec\")\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mvTNFqcyCQ-5",
        "colab_type": "text"
      },
      "source": [
        "# Data Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fE71rzudCO-q",
        "colab_type": "text"
      },
      "source": [
        "The following are done in this section:\n",
        "\n",
        "*   Load Pre-Trained word2vec gensim model, which used Sense Tagged data to train\n",
        "*   Create the Vocab class to create the vocabularies\n",
        "*   Create the NERTaggingData class to create an encoded Dataset\n",
        "*   Get the NER tagged datasets\n",
        "*   Build vocabulary for text and labels --> corresponding indices\n",
        "*   Encode the each sample in the NER tagged datasets ]with its corresponding indices given by the vocabs\n",
        "*   Update the NER tagged datasets to load with the batch size "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uj_BRpeXpysr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Vocab:\n",
        "    def __init__(self, counter, special_tokens, min_freq=1):\n",
        "\n",
        "        # Initiate dictionary and reverse dictionary\n",
        "        self.str2int = defaultdict()\n",
        "        self.int2str = list()\n",
        "        self.special_tokens = special_tokens\n",
        "        \n",
        "        # Add special tokens first to the dictionary\n",
        "        if len(special_tokens) > 0:\n",
        "            for i, word in enumerate(special_tokens):\n",
        "                self.str2int[word] = i\n",
        "                self.int2str.append(word)\n",
        "                \n",
        "                # Set the variable unk_index\n",
        "                if word == '<unk>':\n",
        "                    self.unk_index = i\n",
        "            \n",
        "        \n",
        "        # Add tokens form/label to the dictionaries that have a minimum frequency\n",
        "        counting = 0 + len(special_tokens)\n",
        "        for i, word in enumerate(counter):\n",
        "            if counter[word] >= min_freq:\n",
        "                self.str2int[word] = counting\n",
        "                self.int2str.append(word)\n",
        "                counting += 1\n",
        "\n",
        "      \n",
        "        \n",
        "    def __len__(self):\n",
        "          return len(self.str2int)\n",
        "\n",
        "    # Used to get an item from the dictionaries \n",
        "    def __getitem__(self, key):\n",
        "        if isinstance(key, int):\n",
        "            try:\n",
        "                item = self.int2str[key]\n",
        "            except:\n",
        "                # If out of range, will suggest another value within and return None for the item\n",
        "                item = print('Key index is out of range. Please choose an integer value between 0 and', len(self.str2int))\n",
        "\n",
        "            \n",
        "        if isinstance(key, str):\n",
        "            try:\n",
        "                item = self.str2int[key]\n",
        "            except:\n",
        "                # If string key is not in the dictionary, will return index for UNK - unknown\n",
        "                item = self.str2int['<unk>']            \n",
        "            \n",
        "            \n",
        "        return item\n",
        "\n",
        "    # Similar to the variable used in TorchText Vocab, does it above much simpler without needing to call this variable\n",
        "    def str2int(self):\n",
        "        def __getitem__(self, key):\n",
        "            try:\n",
        "                item = self.str2int[key]\n",
        "            except:\n",
        "                item = self.str2int['<unk>']         \n",
        "        return item\n",
        "\n",
        "    # Similar to the variable used in TorchText Vocab, does it above much simpler without needing to call this variable\n",
        "    def int2str(self):\n",
        "        def __getitem__(self, key):\n",
        "            try:\n",
        "                item = self.int2str[key]\n",
        "            except:\n",
        "                item = print('Key index is out of range. Please choose an integer value between 0 and', len(self.str2int))\n",
        "        return item\n",
        "\n",
        "    # Load the pre-trained vectors into the dictionary. The previous vocab and\n",
        "    # these vectors will have to be aligned to update the integers -> vectors,\n",
        "    # add any missing words, and make 'unk' those words that aren't in the vectors\n",
        "    def load_vectors(self, trained_model):\n",
        "        # Add special tokens first to the dictionary\n",
        "        if len(self.special_tokens) > 0:\n",
        "            if '<unk>' in self.special_tokens: \n",
        "                w2v_model.add('<unk>', np.positive(w2v_model.get_vector('a')*0), replace=False)\n",
        "\n",
        "            for i, word in enumerate(self.special_tokens):\n",
        "                # add special tokens to the word2vec model\n",
        "                change = 0.00001 * i\n",
        "                if word != '<unk>': w2v_model.add(word, np.positive(w2v_model.get_vector('<unk>'))+change,replace=False)\n",
        "\n",
        "        vocab_embed = list((w2v_model.vocab).keys())\n",
        "\n",
        "        return vocab_embed, w2v_model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XW9hfFcIAA4u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Following functions are to add babelnet id for each word that has a POS tag\n",
        "# The word's form is updated with this. This is used for the word sense-embeddings\n",
        "# since these have words as: word_bnID\n",
        "\n",
        "# Get wordnet POS only available Verb, Noun, Adverb, Adjective\n",
        "def get_wordnet_pos(word_tag):\n",
        "    if word_tag.startswith('J'): tag= wn.ADJ\n",
        "    elif word_tag.startswith('V'): tag= wn.VERB\n",
        "    elif word_tag.startswith('N'): tag= wn.NOUN\n",
        "    elif word_tag.startswith('R'): tag= wn.ADV\n",
        "    else: tag= None \n",
        "\n",
        "    return tag\n",
        "\n",
        "# Get 1) lemma from word and POS tag 2) synset using lemma and POS\n",
        "# 3) wordnet number with sysnet offset and pos 4) babelnet number from \n",
        "# bn2wn dictionary created from file. Create new form by combining word_bn#    \n",
        "def get_newForm(sentList, dic_wn2bn, word, wnPOS, word_id):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    \n",
        "    if wnPOS != None and (word[0].islower() and word_id != 0): \n",
        "        try:\n",
        "            lemma = lemmatizer.lemmatize(word, pos=wnPOS) \n",
        "            syn = wn.synsets(lemma, pos=wnPOS)[0] # get most common synset        \n",
        "            wsd_syn = lesk(sentList, lemma, pos=wnPOS) #, synsets=syn)\n",
        "            wnNum = 'wn:' + str(syn.offset()).zfill(8) + syn.pos()\n",
        "            try: \n",
        "                bnNum = dic_wn2bn[wnNum]\n",
        "                new_form = lemma + \"_\" + bnNum\n",
        "            except: new_form = 'None'\n",
        "        except: new_form = 'None'\n",
        "    else: new_form = 'None'\n",
        "    \n",
        "    return new_form\n",
        "\n",
        "# tokenize with Sense\n",
        "def Sense_tokenize(sentences):\n",
        "    # Get file wordnet to babelnet dictionary from file\n",
        "    dic_wn2bn = dict()\n",
        "    with open(os.path.join(directory, 'babelnet2wordnet.tsv'), 'r')  as fileBn2Wn:\n",
        "        file_bn2wn = csv.reader(fileBn2Wn, delimiter=\"\\t\", quotechar='\"')\n",
        "        for row in file_bn2wn:\n",
        "              dic_wn2bn[row[1]] = row[0]\n",
        "\n",
        "    list_newForm = list()\n",
        "\n",
        "    # Create a list of all the new forms to be later updated in dataset\n",
        "    for sentence in sentences:\n",
        "        sentList = list()\n",
        "        for word_id, word_raw in enumerate(list(sentence)):\n",
        "            sentList.append(word_raw['form'])\n",
        "        tagged = nltk.pos_tag(sentList)\n",
        "        for word, tag in tagged:\n",
        "            wnPOS = get_wordnet_pos(tag)\n",
        "            new_form = get_newForm(sentList, dic_wn2bn, word, wnPOS, word_id)            \n",
        "            list_newForm.append(new_form)\n",
        "\n",
        "    # Update each form with new form if available\n",
        "    for i, item in enumerate(itertools.chain.from_iterable(sentences)):\n",
        "        if list_newForm[i] != 'None': \n",
        "            item['form'] = list_newForm[i]\n",
        "        else: continue  \n",
        "    \n",
        "    del list_newForm, dic_wn2bn\n",
        "\n",
        "    return sentences"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5gcBi74IOyJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NERTaggingData:\n",
        "\n",
        "    def __init__(self, \n",
        "                 data_file:str, \n",
        "                 window_size:int, \n",
        "                 window_shift:int=-1,\n",
        "                 lowercase=True, \n",
        "                #  device=\"cpu\"):\n",
        "                 device=\"cuda\"):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            data_file: The path to the dataset already tokenized to be loaded.\n",
        "            \n",
        "            window_size:  The maximum number of tokens in a sentence.\n",
        "            \n",
        "            window_shift: The number of tokens to shift the window in the\n",
        "                          sentence. Default value is -1: window will be \n",
        "                          shifted by window_size.\n",
        "            \n",
        "            lowercase:  Whether the text has to be lowercased or not. For embeddings\n",
        "                        with uppercase words, uppercase is useful (put as False)\n",
        "            \n",
        "            device:  device to run tensors (cpu or cuda).\n",
        "            \n",
        "        Output:\n",
        "            encoded_data: List of samples with each token and its NER tag\n",
        "        \"\"\"\n",
        "\n",
        "        self.data_file = data_file\n",
        "        self.window_size = window_size\n",
        "        self.window_shift = window_shift if window_shift > 0 else window_size\n",
        "        # read and parse entire data file\n",
        "        with open(data_file) as reader:\n",
        "            sentences = parse(reader.read())\n",
        "\n",
        "        # lowers each token, if envoked\n",
        "        self.lowercase = lowercase\n",
        "        if self.lowercase:\n",
        "            for item in itertools.chain.from_iterable(sentences):\n",
        "                item[\"form\"] = item[\"form\"].lower()\n",
        "\n",
        "        # In case to test with only lowercase first word in sentence\n",
        "        # else:\n",
        "        #     for item in itertools.chain.from_iterable(sentences):\n",
        "        #         if item['id'] == 0:\n",
        "        #             # continue\n",
        "        #             item[\"form\"] = item[\"form\"].lower()\n",
        "\n",
        "\n",
        "        # add babelnet id to each word, if available, to add meaning given their POS tag\n",
        "        sentences = Sense_tokenize(sentences)\n",
        "\n",
        "        self.device = device\n",
        "        self.data = self.divide_windows(sentences)\n",
        "        self.encoded_data = None\n",
        "\n",
        "    # Get the length of the data\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    # Returns a sample from the encoded dataset given an index\n",
        "    def __getitem__(self, idx):\n",
        "        if self.encoded_data is None:\n",
        "            raise RuntimeError(\"\"\"Have to call 'encode_dataset' on this object\n",
        "            before trying to retrieve elements. In case you want to retrieve raw\n",
        "            elements, use the method get_windowItem(idx)\"\"\")\n",
        "        return self.encoded_data[idx]\n",
        "\n",
        "\n",
        "    # Returns a item widonw in the original data structure (without encoding)\n",
        "    def get_windowItem(self, idx):\n",
        "        return self.data[idx]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def divide_windows(self, sentences):\n",
        "        \"\"\" \n",
        "        Args:\n",
        "            sentences:  list of lists of dictionaries (each represents a parsed word occurrence)\n",
        "\n",
        "        Output:\n",
        "            data: data that is divided in equal amounts of tokens for each sentence.\n",
        "                  based on windows size and shift. If extra tokens, an empty None\n",
        "                  pad is added.\n",
        "        \"\"\"\n",
        "        data = []\n",
        "        for sentence in sentences:\n",
        "            for i in range(0, len(sentence), self.window_shift):\n",
        "                win_size = len(sentence)-i\n",
        "                if win_size >= self.window_size: window = sentence[i:i+self.window_size]\n",
        "                else: window = sentence[i:i+self.window_size] + [None]*(self.window_size - win_size)\n",
        "                data.append(window)\n",
        "        return data\n",
        "\n",
        " \n",
        "    def encode_dataset(self, vocabulary, label_vocabulary):\n",
        "        \"\"\" \n",
        "        Args:\n",
        "            vocabulary: vocabulary with mappings from words to indices and viceversa\n",
        "\n",
        "            label_vocabulary: vocabulary with mappings from a string label \n",
        "                                to its corresponding index and vice versa\n",
        "\n",
        "        Output:\n",
        "            encoded_data: data encoded into inputs and outputs for each window.\n",
        "                          For windows with None items, a pad is added to its label.\n",
        "        \"\"\"\n",
        "        self.encoded_data = list()\n",
        "        for i in range(len(self.data)):\n",
        "            window = self.data[i]\n",
        "            # text/label encoded are created by calling static cuntions in this class\n",
        "            # device = torch.device(self.device if torch.cuda.is_available() else \"cpu\")\n",
        "            encoded_text = torch.LongTensor(self.get_encode_text2index(window, vocabulary)).to(self.device)\n",
        "            encoded_labels = torch.LongTensor(self.get_encode_label(window, label_vocabulary)).to(self.device)\n",
        "\n",
        "            self.encoded_data.append({\"inputs\":encoded_text, \n",
        "                                      \"outputs\":encoded_labels})\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    # Get the encoded label for each item in the from of a dictionary in the window\n",
        "    # If the item is None, the label becomes <pad>\n",
        "    def get_encode_label(window, label_vocabulary):\n",
        "        label_out = list()\n",
        "        for item_dic in window:\n",
        "            if item_dic is not None: \n",
        "                label_out.append(label_vocabulary[item_dic[\"lemma\"]])\n",
        "            else: \n",
        "                label_out.append(label_vocabulary[\"<pad>\"])\n",
        "        return label_out\n",
        "\n",
        "    @staticmethod\n",
        "    # Get the encoded index of the text for each item in the from of a dictionary in the window\n",
        "    # If the item is None, the label becomes <pad>\n",
        "    def get_encode_text2index(window, vocabulary):\n",
        "        indices = list()\n",
        "        for item_dic in window:\n",
        "            if item_dic is None: \n",
        "                indices.append(vocabulary.str2int[\"<pad>\"])\n",
        "            # if word exists in vocabulary after converting the word string to integer\n",
        "            elif item_dic[\"form\"] in vocabulary.str2int: \n",
        "                indices.append(vocabulary.str2int[item_dic[\"form\"]])\n",
        "            # if word doesn't exist in vocabulary, add UNK indices for unknown\n",
        "            else: indices.append(vocabulary.unk_index) \n",
        "                \n",
        "        return indices\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    # Get a list of the decoded text from the output of NN model in the from of a tensor\n",
        "    def get_decode_index2text(model_outputs, label_vocabulary: Vocab):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            model_output:   a Tensor with shape (batch_size, max_len, label_vocab_size)\n",
        "                            containing the output of the neural network model.\n",
        "\n",
        "            label_vocabulary: vocabulary with mappings from a string label \n",
        "                                to its corresponding index and vice versa\n",
        "\n",
        "        Output:\n",
        "            The method returns a list of batch_size length where each element is a list\n",
        "            of labels, one for each input token.\n",
        "        \"\"\"          \n",
        "\n",
        "        max_indices = torch.argmax(model_outputs, -1).tolist() # shape = (batch_size, max_len)\n",
        "        predictions = list()\n",
        "\n",
        "        # get word text in vocabulary after converting the index integer to word string\n",
        "        for indices in max_indices:\n",
        "            # print([l_label_vocabulary.int2str[i] for i in indices])\n",
        "            predictions.append([label_vocabulary.int2str(i) for i in indices])\n",
        "        return predictions\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4C5AdNFiDPzo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Build a vocabulary for each word and label found as a token dictionary\n",
        "def build_vocab(dataset, w2v_model, min_freq=1):\n",
        "    counter_vocab = Counter()\n",
        "    counter_label = Counter()\n",
        "\n",
        "    for i in tqdm(range(len(dataset))):\n",
        "        for token in dataset.get_windowItem(i):\n",
        "            if token is not None:\n",
        "                counter_vocab[token[\"form\"]] += 1\n",
        "                counter_label[token[\"lemma\"]] += 1\n",
        "    \n",
        "    # Special tokens added for padding and unknown words at testing time\n",
        "    vocabulary = Vocab(counter_vocab, special_tokens=['<pad>', '<unk>'], min_freq=min_freq)\n",
        "    print('Vocabulary length is:  ', len(vocabulary))\n",
        "    if w2v_model != None:\n",
        "        vocab_embed, w2v_model = vocabulary.load_vectors(w2v_model)\n",
        "        print('Updated embedding with the special tokens.')\n",
        "    # No <unk> token for labels\n",
        "    label_vocabulary = Vocab(counter_label, special_tokens=['<pad>'])\n",
        "\n",
        "    return vocabulary, label_vocabulary, vocab_embed, w2v_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oYhcersMKaiU",
        "colab_type": "code",
        "outputId": "dc5c7d42-bd07-4d2d-a0ba-4e82348b057f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        }
      },
      "source": [
        "window_size, window_shift = 100, 100\n",
        "\n",
        "# Get the NER tagged datasets\n",
        "train_dataset = NERTaggingData(file_train, window_size, window_shift, lowercase=False)\n",
        "dev_dataset = NERTaggingData(file_dev, window_size, window_shift, lowercase=False)\n",
        "test_dataset = NERTaggingData(file_test, window_size, window_shift, lowercase=False)\n",
        "\n",
        "\n",
        "\n",
        "nameFiles = [\"vocabulary.pkl\", \"label_vocabulary.pkl\", \"pretrained_embed.pkl\"]\n",
        "savedFiles = [idx for idx in os.listdir(model_dir) if idx.endswith('.pkl')]\n",
        "\n",
        "if all(elem in savedFiles  for elem in nameFiles): \n",
        "    embed_dim = 400\n",
        "\n",
        "    # Load vocabulary\n",
        "    with open(os.path.join(model_dir, \"vocabulary.pkl\"),\"rb\") as file:  \n",
        "        vocabulary = pickle.load(file)\n",
        "\n",
        "    with open(os.path.join(model_dir, \"label_vocabulary.pkl\"),\"rb\") as file:  \n",
        "        label_vocabulary = pickle.load(file)\n",
        "\n",
        "    # Get pre-trained embedings\n",
        "    with open(os.path.join(model_dir, \"pretrained_embed.pkl\"),\"rb\") as file:  \n",
        "        pretrained_embed = pickle.load(file)\n",
        "\n",
        "else:\n",
        "\n",
        "    # Load the word2vec model that has been pre-trained, and get its vocabulary\n",
        "    # Get the weights of the embeddings to use later in the NER tag model\n",
        "    w2v_model = KeyedVectors.load_word2vec_format(embed_file, binary = False)\n",
        "    # embed_weights = torch.FloatTensor(w2v_model.vectors)\n",
        "    vocab_embed = list((w2v_model.vocab).keys())\n",
        "    embed_dim = w2v_model.vector_size\n",
        "\n",
        "    # Build vocabulary for text and labels --> corresponding indices\n",
        "    vocabulary, label_vocabulary, vocab_embed, w2v_model = build_vocab(train_dataset, w2v_model, min_freq=5)\n",
        "\n",
        "    # Get pre-trained embedings\n",
        "    pretrained_embed = torch.randn(len(vocabulary), embed_dim)\n",
        "    initialized = 0\n",
        "    for i, word in enumerate(vocabulary.int2str):\n",
        "        if word in vocab_embed:\n",
        "            initialized += 1\n",
        "            pretrained_embed[i] = torch.from_numpy(w2v_model[word].copy())\n",
        "    print('Amount of initialized pre-trained embedings are: ', initialized)\n",
        "\n",
        "    # Save the vocabularies\n",
        "    with open(os.path.join(model_dir,\"vocabulary.pkl\"),\"wb+\") as file:  \n",
        "        pickle.dump(vocabulary, file)\n",
        "\n",
        "    with open(os.path.join(model_dir,\"label_vocabulary.pkl\"),\"wb+\") as file:  \n",
        "        pickle.dump(label_vocabulary, file)\n",
        "\n",
        "    with open(os.path.join(model_dir,\"pretrained_embed.pkl\"),\"wb+\") as file:  \n",
        "        pickle.dump(pretrained_embed, file)\n",
        "\n",
        "    print('Items saved..')\n",
        "\n",
        "    del w2v_model, vocab_embed\n",
        "\n",
        "\n",
        "\n",
        "# Encode the dataset each sample with its corresponding indices given by the vocabs\n",
        "train_dataset.encode_dataset(vocabulary, label_vocabulary)\n",
        "dev_dataset.encode_dataset(vocabulary, label_vocabulary)\n",
        "test_dataset.encode_dataset(vocabulary, label_vocabulary)\n",
        "\n",
        "# Update the datasets to include the batch size \n",
        "train_dataset = DataLoader(train_dataset, batch_size=128)\n",
        "dev_dataset = DataLoader(dev_dataset, batch_size=128)\n",
        "test_dataset = DataLoader(test_dataset, batch_size=128)\n",
        "\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:253: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n",
            "100%|██████████| 100042/100042 [00:02<00:00, 47055.72it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Vocabulary length is:   23906\n",
            "Updated embedding with the special tokens.\n",
            "Amount of initialized pre-trained embedings are:  8362\n",
            "Items saved..\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
            "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_xOXBsR3te7S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "21AiHGQ41QfA",
        "colab_type": "code",
        "outputId": "1adb46cb-8c57-41b6-b143-91e248589534",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "print(\"car index: \", vocabulary[\"car\"])\n",
        "print(\"<pad> index: \", vocabulary[\"<pad>\"])\n",
        "print(\"<unk> index\", vocabulary[\"<unk>\"])\n",
        "print(\"word at index 154: \", vocabulary.int2str[154])\n",
        "print(\"unknown words are indexed at: \", vocabulary[\"dskvas\"])"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "car index:  1\n",
            "<pad> index:  0\n",
            "<unk> index 1\n",
            "word at index 154:  role_bn:00036823n\n",
            "unknown words are indexed at:  1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QTX2NyCOuDld",
        "colab_type": "text"
      },
      "source": [
        "# Model Building"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-4jLX3m1uG2H",
        "colab_type": "text"
      },
      "source": [
        "The following is included in this section:\n",
        "\n",
        "*   List item\n",
        "*   List item\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZfCgK4fRwRaU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class HypParams():\n",
        "    def __init__(self, vocabulary, label_vocabulary, embed_dim, embeddings=None):\n",
        "        self.vocabulary = vocabulary\n",
        "        self.label_vocabulary = label_vocabulary\n",
        "        self.vocab_size = len(self.vocabulary)\n",
        "        self.hidden_dim = 64\n",
        "        self.embedding_dim = embed_dim\n",
        "        self.num_classes = len(self.label_vocabulary) \n",
        "        self.bidirectional = True\n",
        "        self.num_layers = 2\n",
        "        self.dropout = 0.5\n",
        "        self.embeddings = embeddings\n",
        "model_param = HypParams(vocabulary, label_vocabulary, embed_dim, pretrained_embed)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IRq7VqI9uM-G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NER_Model(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, model_param):\n",
        "        super(NER_Model, self).__init__()\n",
        "\n",
        "        # padding_idx = vocabulary[\"<pad>\"]\n",
        "        # Embedding layer: a matrix (shape: vocab_size, embedding_dim). Where \n",
        "        # each index represents a word\n",
        "        self.word_embedding = torch.nn.Embedding(model_param.vocab_size, \n",
        "                                                 model_param.embedding_dim) #,\n",
        "                                                 #padding_idx=padding_idx)\n",
        "\n",
        "        if model_param.embeddings is not None:\n",
        "            print(\"Initializing embeddings layer from Pre-trained Word Embeddings..\")\n",
        "            self.word_embedding.weight.data.copy_(model_param.embeddings)\n",
        "\n",
        "        # Bi-LSTM \n",
        "        self.lstm = torch.nn.LSTM(model_param.embedding_dim, \n",
        "                                  model_param.hidden_dim, \n",
        "                                  bidirectional = model_param.bidirectional,\n",
        "                                  num_layers = model_param.num_layers, \n",
        "                                  dropout = model_param.dropout)\n",
        "        \n",
        "        lstm_output_dim = model_param.hidden_dim * 2\n",
        "\n",
        "        self.dropout = torch.nn.Dropout(model_param.dropout)\n",
        "        self.classifier = torch.nn.Linear(lstm_output_dim, model_param.num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # print('x in lstm model: ', x.shape)\n",
        "        embeddings = self.word_embedding(x)\n",
        "        # print('embeddings shape: ', str(embeddings.size()))\n",
        "        embeddings = self.dropout(embeddings)\n",
        "        # print('after dropout shape: ', str(embeddings.size()), '\\n')\n",
        "        o, (h, c) = self.lstm(embeddings)\n",
        "        o = self.dropout(o)\n",
        "        output = self.classifier(o)\n",
        "        return output\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PQEFXEgsxR6d",
        "colab_type": "code",
        "outputId": "156c4c79-3f39-4ff9-cb26-57c4fbc6d6b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "nertag_Model = NER_Model(model_param).cuda()\n",
        "nertag_Model"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Initializing embeddings layer from Pre-trained Word Embeddings..\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "NER_Model(\n",
              "  (word_embedding): Embedding(23906, 400)\n",
              "  (lstm): LSTM(400, 64, num_layers=2, bidirectional=True)\n",
              "  (dropout): Dropout(p=0, inplace=False)\n",
              "  (classifier): Linear(in_features=128, out_features=5, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HxTBHQ4wj1jF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4chSCIND4f5p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# print(len(toy_dataset))\n",
        "# encoded_input = [x[\"inputs\"] for x in toy_dataset[:10]]\n",
        "# # print(torch.stack(encoded_input, 0).shape)\n",
        "# logits = nertag_Model(torch.stack(encoded_input, 0)).cuda()\n",
        "# labels = NERTaggingData.get_decode_index2text(logits, label_vocabulary)\n",
        "# for i in range(5):\n",
        "#     i_sentence = toy_dataset.get_windowItem(i)\n",
        "#     i_labels = labels[i]\n",
        "#     print(list(zip([w[\"form\"] for w in i_sentence if w is not None], i_labels)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KHM9Ekgvz0Y1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Trainer:\n",
        "  \n",
        "    def __init__(self, model, loss_function, optimizer, label_vocab):\n",
        "        self.model = model\n",
        "        self.loss_function = loss_function\n",
        "        self.optimizer = optimizer\n",
        "        self.label_vocab = label_vocab\n",
        "\n",
        "    def train(self, train_dataset, valid_dataset, epochs:int=2):\n",
        "\n",
        "        print('Training ...')\n",
        "        train_loss = 0.0\n",
        "        for epoch in range(epochs):\n",
        "            print(' Epoch {:03d}'.format(epoch + 1))\n",
        "\n",
        "            epoch_loss = 0.0\n",
        "            self.model.train()\n",
        "\n",
        "            for step, item in enumerate(train_dataset):\n",
        "                inputs = item['inputs']\n",
        "                labels = item['outputs']\n",
        "                self.optimizer.zero_grad()\n",
        "\n",
        "                #input data to model to train and output the prediction\n",
        "                pred = self.model(inputs)\n",
        "                pred = pred.view(-1, pred.shape[-1])\n",
        "                labels = labels.view(-1)\n",
        "                \n",
        "                loss = self.loss_function(pred, labels)\n",
        "                loss.backward(create_graph=True)\n",
        "                self.optimizer.step()\n",
        "\n",
        "                epoch_loss += loss.tolist()\n",
        "                \n",
        "                # if step % 500 == 0:\n",
        "                #     print('\\tStep {}: current loss = {:0.4f}'.format(step, epoch_loss / (step + 1)))\n",
        "            \n",
        "            avg_epoch_loss = epoch_loss / len(train_dataset)\n",
        "            train_loss += avg_epoch_loss\n",
        "            \n",
        "            print('\\tAverage Train loss = {:0.4f}'.format(avg_epoch_loss))\n",
        "\n",
        "        print('Done Training!')\n",
        "        avg_epoch_loss = train_loss / epochs\n",
        "        \n",
        "        return avg_epoch_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iu_ukDiyah0Z",
        "colab_type": "code",
        "outputId": "0a4e1e69-3805-4de3-e100-6f60a69e1f4f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# weight_dataset = NERTaggingData(file_train, 100, 100, lowercase=False)\n",
        "\n",
        "# counter_label = Counter()\n",
        "\n",
        "# for i in tqdm(range(len(weight_dataset))):\n",
        "#     for token in weight_dataset.get_windowItem(i):\n",
        "#         if token is not None:\n",
        "#             counter_label[token[\"lemma\"]] += 1\n",
        "    \n",
        "# print(counter_label)\n",
        "#  [('<pad>', 0), ('PER', 1), ('O', 2), ('ORG', 3), ('LOC', 4)]\n",
        "# Counter({'<pad>': 0, 'PER': 100409, 'O': 2177423, 'ORG': 61988, 'LOC': 84937})\n",
        "ratio = [0, 2177423/100409, 2177423/2177423, 2177423/61988, 2177423/84937]\n",
        "weight_loss = torch.FloatTensor(ratio)\n",
        "print(weight_loss)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([ 0.0000, 21.6855,  1.0000, 35.1265, 25.6357])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bDDkEzV8ztzf",
        "colab_type": "code",
        "outputId": "3af20225-6cd6-49ef-93e6-a31460532019",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 748
        }
      },
      "source": [
        "trainer = Trainer(\n",
        "    model = nertag_Model,\n",
        "    loss_function = torch.nn.CrossEntropyLoss(ignore_index=label_vocabulary['<pad>'], weight=weight_loss.cuda()),\n",
        "    optimizer = torch.optim.Adam(nertag_Model.parameters()),\n",
        "    # optimizer = torch.optim.Adadelta(nertag_Model.parameters()),\n",
        "    # optimizer = torch.optim.Adagrad(nertag_Model.parameters()),\n",
        "    label_vocab=label_vocabulary)\n",
        "\n",
        "trainer.train(train_dataset, dev_dataset, 20)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training ...\n",
            " Epoch 001\n",
            "\tTrain loss = 0.8227\n",
            " Epoch 002\n",
            "\tTrain loss = 0.6654\n",
            " Epoch 003\n",
            "\tTrain loss = 0.6860\n",
            " Epoch 004\n",
            "\tTrain loss = 0.6472\n",
            " Epoch 005\n",
            "\tTrain loss = 0.5949\n",
            " Epoch 006\n",
            "\tTrain loss = 0.5706\n",
            " Epoch 007\n",
            "\tTrain loss = 0.5511\n",
            " Epoch 008\n",
            "\tTrain loss = 0.5343\n",
            " Epoch 009\n",
            "\tTrain loss = 0.5222\n",
            " Epoch 010\n",
            "\tTrain loss = 0.5150\n",
            " Epoch 011\n",
            "\tTrain loss = 0.5099\n",
            " Epoch 012\n",
            "\tTrain loss = 0.5065\n",
            " Epoch 013\n",
            "\tTrain loss = 0.5043\n",
            " Epoch 014\n",
            "\tTrain loss = 0.5027\n",
            " Epoch 015\n",
            "\tTrain loss = 0.5015\n",
            " Epoch 016\n",
            "\tTrain loss = 0.5003\n",
            " Epoch 017\n",
            "\tTrain loss = 0.4992\n",
            " Epoch 018\n",
            "\tTrain loss = 0.4981\n",
            " Epoch 019\n",
            "\tTrain loss = 0.4971\n",
            " Epoch 020\n",
            "\tTrain loss = 0.4963\n",
            "... Done!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5562688337700903"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ROE98NQu152I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Save model weights\n",
        "# model_name = '06_partPOStag_capTot_embed_Bi-LSTM'\n",
        "model_name = '07_partPOStag_capTot_embed_Bi-LSTM'\n",
        "saved_model_path = os.path.join(model_dir, model_name + \".pt\")\n",
        "output = open(saved_model_path, mode=\"wb+\")\n",
        "torch.save(nertag_Model.state_dict(), output)\n",
        "output.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y783Sid7uVTj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Computes the performance of the model for different datasets needed\n",
        "def compute_performance(model, dataset, label_vocab):\n",
        "    predictions = list()\n",
        "    labels = list()\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for item in dataset:\n",
        "            inputs = item[\"inputs\"]\n",
        "            outputs = item[\"outputs\"].tolist()\n",
        "\n",
        "            prediction = model(inputs)\n",
        "            prediction = torch.argmax(prediction, -1).tolist() \n",
        "\n",
        "            for i, item in enumerate(outputs):\n",
        "                max_index = len(item) - item.count(0)\n",
        "                labels += item[:max_index]\n",
        "                predictions += prediction[i][:max_index]\n",
        "                # print(prediction[i][:max_index])\n",
        "                # print(item[:max_index], '\\n')\n",
        "\n",
        "\n",
        "    # macro precision, recall, f1: computes metric and then averages across class.  \n",
        "    # Doesn't take into account the quantity of samples per class\n",
        "    macro_precision = precision_score(labels, predictions, average=\"macro\", zero_division=0)\n",
        "    macro_recall = recall_score(labels, predictions, average=\"macro\", zero_division=0)\n",
        "    macro_f1 = f1_score(labels, predictions, average=\"macro\", zero_division=0)\n",
        "\n",
        "    # To check the precision per class\n",
        "    per_class_f1 = f1_score(labels, predictions, labels = list(i for i, item in enumerate(label_vocabulary.str2int)), average=None, zero_division=0)\n",
        "\n",
        "    # Confusion matrix\n",
        "    confusionMatrix = confusion_matrix(labels, predictions, labels = [i for i in label_vocabulary.str2int.values()][1:], normalize='true')\n",
        "\n",
        "\n",
        "    output_performance = {\"macro_precision\":macro_precision,\n",
        "                          \"macro_recall\":macro_recall, \n",
        "                          \"macro_f1\":macro_f1,\n",
        "                          \"per_class_f1\":per_class_f1,\n",
        "                          \"confusionMatrix\":confusionMatrix}\n",
        "\n",
        "    return output_performance\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DJuO9annt77j",
        "colab_type": "code",
        "outputId": "043a210a-1c43-4822-c093-8cec53fd145e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "# Gets the performance for train, dev, and testing\n",
        "# Then, plots a table with recall, precision and f1 (overall and per class) \n",
        "\n",
        "performance_train = compute_performance(nertag_Model, train_dataset, label_vocabulary)\n",
        "performance_dev = compute_performance(nertag_Model, dev_dataset, label_vocabulary)\n",
        "performance_test = compute_performance(nertag_Model, test_dataset, label_vocabulary)\n",
        "\n",
        "# Overall Precision, Recall and F1\n",
        "print(\"Performance Type\\tTrain\\t\\tValidation\\tTest\")\n",
        "print(\"=\"*65)\n",
        "print(\"Precision:\\t\\t{:0.3f}\\t\\t{:0.3f}\\t\\t{:0.3f}\".format(performance_train[\"macro_precision\"], performance_dev[\"macro_precision\"], performance_test[\"macro_precision\"]))\n",
        "print(\"Recall:\\t\\t\\t{:0.3f}\\t\\t{:0.3f}\\t\\t{:0.3f}\".format(performance_train[\"macro_recall\"], performance_dev[\"macro_recall\"], performance_test[\"macro_recall\"]))\n",
        "print(\"F1:\\t\\t\\t{:0.3f}\\t\\t{:0.3f}\\t\\t{:0.3f}\".format(performance_train[\"macro_f1\"], performance_dev[\"macro_f1\"], performance_test[\"macro_f1\"]))\n",
        "print(\"-\"*65)\n",
        "print('F1 per Class')\n",
        "\n",
        "# Per Class F1\n",
        "train_perClass = list()\n",
        "dev_perClass = list()\n",
        "test_perClass = list()\n",
        "#train\n",
        "for id_class, performance in sorted(enumerate(performance_train[\"per_class_f1\"]), key=lambda elem: -elem[1]):\n",
        "    label = label_vocabulary.int2str[id_class]\n",
        "    train_perClass += [label, performance]\n",
        "#dev\n",
        "for id_class, performance in sorted(enumerate(performance_dev[\"per_class_f1\"]), key=lambda elem: -elem[1]):\n",
        "    label = label_vocabulary.int2str[id_class]\n",
        "    dev_perClass += [label, performance]\n",
        "#test\n",
        "for id_class, performance in sorted(enumerate(performance_test[\"per_class_f1\"]), key=lambda elem: -elem[1]):\n",
        "    label = label_vocabulary.int2str[id_class]\n",
        "    test_perClass += [label, performance]\n",
        "\n",
        "# Place results of per Class in table\n",
        "labels = ['O', 'PER', 'LOC', 'ORG']\n",
        "for item in labels:\n",
        "    print(\"\\t{}:\\t\\t{:0.3f}\\t\\t{:0.3f}\\t\\t{:0.3f}\".format(item, train_perClass[train_perClass.index(item)+1], dev_perClass[dev_perClass.index(item)+1], test_perClass[test_perClass.index(item)+1]))\n",
        "\n"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Performance Type\tTrain\t\tValidation\tTest\n",
            "=================================================================\n",
            "Precision:\t\t0.561\t\t0.547\t\t0.551\n",
            "Recall:\t\t\t0.775\t\t0.753\t\t0.758\n",
            "F1:\t\t\t0.611\t\t0.594\t\t0.599\n",
            "-----------------------------------------------------------------\n",
            "F1 per Class\n",
            "\tO:\t\t0.934\t\t0.931\t\t0.931\n",
            "\tPER:\t\t0.728\t\t0.703\t\t0.706\n",
            "\tLOC:\t\t0.436\t\t0.418\t\t0.432\n",
            "\tORG:\t\t0.347\t\t0.323\t\t0.327\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q3XN9XYNsqGR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plots a heat map confusion matrix for each train, dev, test\n",
        "\n",
        "# Confusion Matrix for TRAINING\n",
        "df_confusionMatrix = pd.DataFrame(performance_train[\"confusionMatrix\"], index = [i for i in label_vocabulary.int2str[1:]],\n",
        "                  columns = [i for i in label_vocabulary.int2str[1:]])\n",
        "ax = plt.axes()\n",
        "sn.heatmap(df_confusionMatrix, annot=True, annot_kws={\"size\": 11}, cmap='Oranges', ax = ax, fmt='.2%')\n",
        "ax.set_title('NER Normalized Confusion Matrix for Training Data', pad=20)\n",
        "ax.set_xlabel('Predicted Labels')\n",
        "ax.set_ylabel('Ground Truth Labels')\n",
        "ax.set_xticklabels(ax.get_xticklabels(), rotation = 25, fontsize = 10)\n",
        "ax.set_yticklabels(ax.get_yticklabels(), rotation = 25, fontsize = 10)\n",
        "plt.savefig('images/CM_train.png', bbox_inches='tight', pad_inches=0.2)\n",
        "plt.show()\n",
        "\n",
        "# Confusion Matrix for VALIDATION\n",
        "df_confusionMatrix = pd.DataFrame(performance_dev[\"confusionMatrix\"], index = [i for i in label_vocabulary.int2str[1:]],\n",
        "                  columns = [i for i in label_vocabulary.int2str[1:]])\n",
        "ax = plt.axes()\n",
        "sn.heatmap(df_confusionMatrix, annot=True, annot_kws={\"size\": 11}, cmap='Blues', ax = ax, fmt='.2%')\n",
        "ax.set_title('NER Normalized Confusion Matrix for Validation Data', pad=20)\n",
        "ax.set_xlabel('Predicted Labels')\n",
        "ax.set_ylabel('Ground Truth Labels')\n",
        "ax.set_xticklabels(ax.get_xticklabels(), rotation = 25, fontsize = 10)\n",
        "ax.set_yticklabels(ax.get_yticklabels(), rotation = 25, fontsize = 10)\n",
        "plt.savefig('images/CM_dev.png', bbox_inches='tight', pad_inches=0.2)\n",
        "plt.show()\n",
        "\n",
        "# Confusion Matrix for TESTING\n",
        "df_confusionMatrix = pd.DataFrame(performance_test[\"confusionMatrix\"], index = [i for i in label_vocabulary.int2str[1:]],\n",
        "                  columns = [i for i in label_vocabulary.int2str[1:]])\n",
        "ax = plt.axes()\n",
        "sn.heatmap(df_confusionMatrix, annot=True, annot_kws={\"size\": 11}, cmap='Greens', ax = ax, fmt='.2%')\n",
        "ax.set_title('NER Normalized Confusion Matrix for Testing Data', pad=20)\n",
        "ax.set_xlabel('Predicted Labels')\n",
        "ax.set_ylabel('Ground Truth Labels')\n",
        "ax.set_xticklabels(ax.get_xticklabels(), rotation = 25, fontsize = 10)\n",
        "ax.set_yticklabels(ax.get_yticklabels(), rotation = 25, fontsize = 10)\n",
        "plt.savefig('images/CM_test.png', bbox_inches='tight', pad_inches=0.2)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}